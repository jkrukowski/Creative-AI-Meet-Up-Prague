{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI + Sound"
      ],
      "metadata": {
        "id": "NP0FzI0MZ9go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"This is Google Colab!\")"
      ],
      "metadata": {
        "id": "GNhfS1_lgsLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "AFrjFsJc_XEx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3sQzP3P9uD9"
      },
      "outputs": [],
      "source": [
        "!pip install spleeter\n",
        "!pip install diffusers\n",
        "!pip install pydub\n",
        "\n",
        "!wget -O image_example.jpeg https://filesamples.com/samples/image/jpeg/sample_1280%C3%97853.jpeg\n",
        "!wget https://github.com/deezer/spleeter/raw/master/audio_example.mp3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "R0JmZT_bb5cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "2LiTmu7N_wem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "from diffusers import AudioLDM2Pipeline\n",
        "import scipy\n",
        "import uuid\n",
        "import typing as T\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import pydub\n",
        "import torch\n",
        "import torchaudio\n",
        "from PIL import Image\n",
        "from scipy.io import wavfile"
      ],
      "metadata": {
        "id": "SQ96DjEp_ylO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image to Sound\n",
        "\n",
        "Inspired by Riffusion https://huggingface.co/riffusion/riffusion-model-v1"
      ],
      "metadata": {
        "id": "CFDhnufMGYp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper functions\n",
        "def image_from_spectrogram(spectrogram: np.ndarray, power: float = 0.25) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Compute a spectrogram image from a spectrogram magnitude array.\n",
        "    This is the inverse of spectrogram_from_image, except for discretization error from\n",
        "    quantizing to uint8.\n",
        "    Args:\n",
        "        spectrogram: (channels, frequency, time)\n",
        "        power: A power curve to apply to the spectrogram to preserve contrast\n",
        "    Returns:\n",
        "        image: (frequency, time, channels)\n",
        "    \"\"\"\n",
        "    # Rescale to 0-1\n",
        "    max_value = np.max(spectrogram)\n",
        "    data = spectrogram / max_value\n",
        "\n",
        "    # Apply the power curve\n",
        "    data = np.power(data, power)\n",
        "\n",
        "    # Rescale to 0-255\n",
        "    data = data * 255\n",
        "\n",
        "    # Invert\n",
        "    data = 255 - data\n",
        "\n",
        "    # Convert to uint8\n",
        "    data = data.astype(np.uint8)\n",
        "\n",
        "    # Munge channels into a PIL image\n",
        "    if data.shape[0] == 1:\n",
        "        # TODO(hayk): Do we want to write single channel to disk instead?\n",
        "        image = Image.fromarray(data[0], mode=\"L\").convert(\"RGB\")\n",
        "    elif data.shape[0] == 2:\n",
        "        data = np.array([np.zeros_like(data[0]), data[0], data[1]]).transpose(1, 2, 0)\n",
        "        image = Image.fromarray(data, mode=\"RGB\")\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Unsupported number of channels: {data.shape[0]}\")\n",
        "\n",
        "    # Flip Y\n",
        "    image = image.transpose(Image.Transpose.FLIP_TOP_BOTTOM)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def spectrogram_from_image(\n",
        "    image: Image.Image,\n",
        "    power: float = 0.25,\n",
        "    stereo: bool = False,\n",
        "    max_value: float = 30e6,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute a spectrogram magnitude array from a spectrogram image.\n",
        "    This is the inverse of image_from_spectrogram, except for discretization error from\n",
        "    quantizing to uint8.\n",
        "    Args:\n",
        "        image: (frequency, time, channels)\n",
        "        power: The power curve applied to the spectrogram\n",
        "        stereo: Whether the spectrogram encodes stereo data\n",
        "        max_value: The max value of the original spectrogram. In practice doesn't matter.\n",
        "    Returns:\n",
        "        spectrogram: (channels, frequency, time)\n",
        "    \"\"\"\n",
        "    # Convert to RGB if single channel\n",
        "    if image.mode in (\"P\", \"L\"):\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    # Flip Y\n",
        "    # image = image.transpose(Image.Transpose.FLIP_TOP_BOTTOM)\n",
        "\n",
        "    # Munge channels into a numpy array of (channels, frequency, time)\n",
        "    data = np.array(image).transpose(2, 0, 1)\n",
        "    if stereo:\n",
        "        # Take the G and B channels as done in image_from_spectrogram\n",
        "        data = data[[1, 2], :, :]\n",
        "    else:\n",
        "        data = data[0:1, :, :]\n",
        "\n",
        "    # Convert to floats\n",
        "    data = data.astype(np.float32)\n",
        "\n",
        "    # Invert\n",
        "    data = 255 - data\n",
        "\n",
        "    # Rescale to 0-1\n",
        "    data = data / 255\n",
        "\n",
        "    # Reverse the power curve\n",
        "    data = np.power(data, 1 / power)\n",
        "\n",
        "    # Rescale to max value\n",
        "    data = data * max_value\n",
        "\n",
        "    return data\n",
        "\n",
        "def wav_bytes_from_spectrogram_image(image: Image.Image) -> T.Tuple[io.BytesIO, float]:\n",
        "    \"\"\"\n",
        "    Reconstruct a WAV audio clip from a spectrogram image. Also returns the duration in seconds.\n",
        "    \"\"\"\n",
        "\n",
        "    max_volume = 50\n",
        "    power_for_image = 0.25\n",
        "    Sxx = spectrogram_from_image(image, max_volume=max_volume, power_for_image=power_for_image)\n",
        "\n",
        "    sample_rate = 44100  # [Hz]\n",
        "    clip_duration_ms = 5000  # [ms]\n",
        "\n",
        "    bins_per_image = 512\n",
        "    n_mels = 512\n",
        "\n",
        "    # FFT parameters\n",
        "    window_duration_ms = 100  # [ms]\n",
        "    padded_duration_ms = 400  # [ms]\n",
        "    step_size_ms = 10  # [ms]\n",
        "\n",
        "    # Derived parameters\n",
        "    num_samples = int(image.width / float(bins_per_image) * clip_duration_ms) * sample_rate\n",
        "    n_fft = int(padded_duration_ms / 1000.0 * sample_rate)\n",
        "    hop_length = int(step_size_ms / 1000.0 * sample_rate)\n",
        "    win_length = int(window_duration_ms / 1000.0 * sample_rate)\n",
        "\n",
        "    samples = waveform_from_spectrogram(\n",
        "        Sxx=Sxx,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        win_length=win_length,\n",
        "        num_samples=num_samples,\n",
        "        sample_rate=sample_rate,\n",
        "        mel_scale=True,\n",
        "        n_mels=n_mels,\n",
        "        max_mel_iters=200,\n",
        "        num_griffin_lim_iters=32,\n",
        "    )\n",
        "\n",
        "    wav_bytes = io.BytesIO()\n",
        "    wavfile.write(wav_bytes, sample_rate, samples.astype(np.int16))\n",
        "    wav_bytes.seek(0)\n",
        "\n",
        "    duration_s = float(len(samples)) / sample_rate\n",
        "\n",
        "    return wav_bytes, duration_s\n",
        "\n",
        "\n",
        "def spectrogram_from_image(\n",
        "    image: Image.Image, max_volume: float = 50, power_for_image: float = 0.25\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute a spectrogram magnitude array from a spectrogram image.\n",
        "\n",
        "    TODO(hayk): Add image_from_spectrogram and call this out as the reverse.\n",
        "    \"\"\"\n",
        "    # Convert to a numpy array of floats\n",
        "    data = np.array(image).astype(np.float32)\n",
        "\n",
        "    # Flip Y take a single channel\n",
        "    data = data[::-1, :, 0]\n",
        "\n",
        "    # Invert\n",
        "    data = 255 - data\n",
        "\n",
        "    # Rescale to max volume\n",
        "    data = data * max_volume / 255\n",
        "\n",
        "    # Reverse the power curve\n",
        "    data = np.power(data, 1 / power_for_image)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def spectrogram_from_waveform(\n",
        "    waveform: np.ndarray,\n",
        "    sample_rate: int,\n",
        "    n_fft: int,\n",
        "    hop_length: int,\n",
        "    win_length: int,\n",
        "    mel_scale: bool = True,\n",
        "    n_mels: int = 512,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute a spectrogram from a waveform.\n",
        "    \"\"\"\n",
        "\n",
        "    spectrogram_func = torchaudio.transforms.Spectrogram(\n",
        "        n_fft=n_fft,\n",
        "        power=None,\n",
        "        hop_length=hop_length,\n",
        "        win_length=win_length,\n",
        "    )\n",
        "\n",
        "    waveform_tensor = torch.from_numpy(waveform.astype(np.float32)).reshape(1, -1)\n",
        "    Sxx_complex = spectrogram_func(waveform_tensor).numpy()[0]\n",
        "\n",
        "    Sxx_mag = np.abs(Sxx_complex)\n",
        "\n",
        "    if mel_scale:\n",
        "        mel_scaler = torchaudio.transforms.MelScale(\n",
        "            n_mels=n_mels,\n",
        "            sample_rate=sample_rate,\n",
        "            f_min=0,\n",
        "            f_max=10000,\n",
        "            n_stft=n_fft // 2 + 1,\n",
        "            norm=None,\n",
        "            mel_scale=\"htk\",\n",
        "        )\n",
        "\n",
        "        Sxx_mag = mel_scaler(torch.from_numpy(Sxx_mag)).numpy()\n",
        "\n",
        "    return Sxx_mag\n",
        "\n",
        "\n",
        "def waveform_from_spectrogram(\n",
        "    Sxx: np.ndarray,\n",
        "    n_fft: int,\n",
        "    hop_length: int,\n",
        "    win_length: int,\n",
        "    num_samples: int,\n",
        "    sample_rate: int,\n",
        "    mel_scale: bool = True,\n",
        "    n_mels: int = 512,\n",
        "    max_mel_iters: int = 200,\n",
        "    num_griffin_lim_iters: int = 32,\n",
        "    device: str = \"cpu\",\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Reconstruct a waveform from a spectrogram.\n",
        "\n",
        "    This is an approximate inverse of spectrogram_from_waveform, using the Griffin-Lim algorithm\n",
        "    to approximate the phase.\n",
        "    \"\"\"\n",
        "    Sxx_torch = torch.from_numpy(Sxx).to(device)\n",
        "\n",
        "    # TODO(hayk): Make this a class that caches the two things\n",
        "\n",
        "    if mel_scale:\n",
        "        mel_inv_scaler = torchaudio.transforms.InverseMelScale(\n",
        "            n_mels=n_mels,\n",
        "            sample_rate=sample_rate,\n",
        "            f_min=0,\n",
        "            f_max=10000,\n",
        "            n_stft=n_fft // 2 + 1,\n",
        "            norm=None,\n",
        "            mel_scale=\"htk\",\n",
        "            # max_iter=max_mel_iters,\n",
        "        ).to(device)\n",
        "\n",
        "        Sxx_torch = mel_inv_scaler(Sxx_torch)\n",
        "\n",
        "    griffin_lim = torchaudio.transforms.GriffinLim(\n",
        "        n_fft=n_fft,\n",
        "        win_length=win_length,\n",
        "        hop_length=hop_length,\n",
        "        power=1.0,\n",
        "        n_iter=num_griffin_lim_iters,\n",
        "    ).to(device)\n",
        "\n",
        "    waveform = griffin_lim(Sxx_torch).cpu().numpy()\n",
        "\n",
        "    return waveform\n",
        "\n",
        "\n",
        "def save_to_wav(wav_bytes: io.BytesIO):\n",
        "  file_name = f\"{uuid.uuid4()}.wav\"\n",
        "  sound = pydub.AudioSegment.from_wav(wav_bytes)\n",
        "  sound.export(file_name)\n",
        "  return file_name"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dGkDJs_BGfqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with Image.open(\"image_example.jpeg\") as image:\n",
        "  spec = spectrogram_from_image(image)\n",
        "  assert(len(spec.shape) > 1)\n",
        "  if len(spec.shape) == 2:\n",
        "    spec = spec[np.newaxis, ...]"
      ],
      "metadata": {
        "id": "7ZdJoUzLG1cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(spec[0], interpolation='nearest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9AzRAS8tHFA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sound_file = save_to_wav(wav_bytes_from_spectrogram_image(image_from_spectrogram(spec[:, :512, :]))[0])"
      ],
      "metadata": {
        "id": "a_log6nwHLer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Audio(sound_file))"
      ],
      "metadata": {
        "id": "1Nmdgo3iHQoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spleeter\n",
        "\n",
        "https://github.com/deezer/spleeter\n",
        "\n",
        "Spleeter is Deezer source separation library with pretrained models written in Python and uses Tensorflow. It makes it easy to train source separation model (assuming you have a dataset of isolated sources), and provides already trained state of the art model for performing various flavour of separation"
      ],
      "metadata": {
        "id": "dRzODVZW_gne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_to_split = \"audio_example.mp3\""
      ],
      "metadata": {
        "id": "W0G2kg8O_leq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Audio(file_to_split))"
      ],
      "metadata": {
        "id": "TUD_Rs-4BeVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spleeter separate -p spleeter:4stems -o output {file_to_split}"
      ],
      "metadata": {
        "id": "lfreko5K_jWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Audio(\"output/audio_example/bass.wav\"))\n",
        "display(Audio(\"output/audio_example/drums.wav\"))\n",
        "display(Audio(\"output/audio_example/other.wav\"))\n",
        "display(Audio(\"output/audio_example/vocals.wav\"))"
      ],
      "metadata": {
        "id": "2vJVxnUwBrCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AudioLDM 2\n",
        "\n",
        "https://github.com/haoheliu/AudioLDM2"
      ],
      "metadata": {
        "id": "7JlrKqb6DIRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"cvssp/audioldm2\"\n",
        "pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "snWXrgMhDR9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Musical constellations twinkling in the night sky, forming a cosmic melody.\"\n",
        "negative_prompt = \"Low quality\""
      ],
      "metadata": {
        "id": "2qXkWbnDDLnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio = pipe(\n",
        "    prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    num_inference_steps=200,\n",
        "    audio_length_in_s=10.0,\n",
        "    num_waveforms_per_prompt=3\n",
        ").audios"
      ],
      "metadata": {
        "id": "jKdF6egMDX2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = f\"{uuid.uuid4()}.wav\"\n",
        "scipy.io.wavfile.write(file_name, rate=16000, data=audio[0])"
      ],
      "metadata": {
        "id": "t_mphg5DDfsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Audio(file_name))"
      ],
      "metadata": {
        "id": "sU71xPv5Dife"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  "
      ],
      "metadata": {
        "id": "T_sVVhdYC9T6"
      }
    }
  ]
}